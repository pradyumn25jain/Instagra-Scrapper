{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "from itertools import cycle\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionid = '39673737047%3Acf6Ydg9lghnde6%3A23'\n",
    "h = {'Accept-Language':'en-US,en;q=0.5',\"Cookie\":\"sessionid={}\".format(sessionid)}\n",
    "# h['x-csrftoken']=\"DkPugukmMfCunkG7pDS6dS0T6wnZQqBX\"\n",
    "# h['content-type']='application/x-www-form-urlencoded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Requests:\n",
    "    use_proxy = None\n",
    "    proxy_handler = None\n",
    "    def __init__(self,use_proxy_param=False,use_session=True):\n",
    "        if use_proxy_param:\n",
    "            proxy_handler = PROXYHANDLER()\n",
    "            self.proxy_handler = proxy_handler\n",
    "        if use_session:\n",
    "            self.session = HTMLSession()\n",
    "        else:\n",
    "            self.session = requests\n",
    "            \n",
    "    def get(self,url,parser=\"html.parser\",request_type='get',headers=None\n",
    "                     ,payload=None,params=None,try_times=20):\n",
    "        retry_times = 0\n",
    "        response = None\n",
    "        while retry_times<=try_times:\n",
    "            try:\n",
    "                if self.proxy_handler:\n",
    "                    proxy_option = self.proxy_handler.make_proxy()\n",
    "                else:\n",
    "                    proxy_option = None\n",
    "                if request_type=='get':\n",
    "                    response = self.session.get(url,proxies=proxy_option,headers=headers,\n",
    "                                                data=payload,params=params)\n",
    "                else:\n",
    "                    response = self.session.post(url,proxies=proxy_option,headers=headers,\n",
    "                                                data=payload,params=params)\n",
    "                if response.ok:\n",
    "                    break\n",
    "            except:\n",
    "                #traceback.print_exc()\n",
    "                retry_times = retry_times + 1\n",
    "        if response:\n",
    "            if parser == 'json':\n",
    "                response = response.json()\n",
    "            elif parser == 'content':\n",
    "                response = response.content\n",
    "            elif parser == 'html.parser':\n",
    "                response = BeautifulSoup(response.content,parser)\n",
    "            elif parser == 'xml':\n",
    "                response = BeautifulSoup(response.content,parser)\n",
    "            else:\n",
    "                response = response.content\n",
    "        return response\n",
    "\n",
    "class PROXYHANDLER():\n",
    "    proxy_cycle = None\n",
    "    proxies_list = []\n",
    "    proxy_used = 0\n",
    "    IP = None\n",
    "    \n",
    "    def __init__(self,recall_proxy_interval = 25,flag=False):\n",
    "        self.flag = flag\n",
    "        self.recall_proxy_interval = recall_proxy_interval\n",
    "        try:    \n",
    "            self.IP = self.get_ip_API()\n",
    "        except:\n",
    "            self.IP = self.get_ip()\n",
    "        self.add_ip_to_whitelist(self.IP)\n",
    "        self._call_for_tt_proxies()\n",
    "\n",
    "\n",
    "    def _check_to_call_tt_proxy(self):\n",
    "        if self.proxy_used % self.recall_proxy_interval == 0:\n",
    "            self._call_for_tt_proxies()\n",
    "\n",
    "\n",
    "    def _call_for_tt_proxies(self):\n",
    "        try_times = 10\n",
    "        while try_times != 0:\n",
    "            try:\n",
    "                self.obtain_proxy_list_from_tt_proxy()\n",
    "                self.shuffel_proxies()\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                try_times = try_times - 1\n",
    "\n",
    "\n",
    "    def shuffel_proxies(self,flag=False):\n",
    "        random.shuffle(self.proxies_list)\n",
    "        self.proxy_cycle = cycle(self.proxies_list)\n",
    "\n",
    "\n",
    "    def make_proxy(self):\n",
    "        PROXY = next(self.proxy_cycle)\n",
    "        self.proxy_used = self.proxy_used + 1\n",
    "        self._check_to_call_tt_proxy()\n",
    "        return PROXY\n",
    "\n",
    "    def get_proxy_list_from_tt_proxy(self):\n",
    "        proxies = []\n",
    "        secret = '50piVRZ9ZedEy2EfqXX9u3'\n",
    "        params = {\n",
    "            \"license\": \"PBA5EE66C8FE5DE84\",\n",
    "            \"time\": int(time.time()),\n",
    "            \"cnt\": 1000,\n",
    "        }\n",
    "        params[\"sign\"] = hashlib.md5((params[\"license\"] + str(params[\"time\"]) + secret).encode('utf-8')).hexdigest()\n",
    "        response = requests.get(\n",
    "                    url=\"https://api.ttproxy.com/v1/obtain\",\n",
    "                    params=params,\n",
    "                    headers={\n",
    "                        \"Content-Type\": \"text/plain; charset=utf-8\",\n",
    "                    },\n",
    "                    data=\"1\"\n",
    "                ).json()\n",
    "        proxies = response[\"data\"][\"proxies\"]\n",
    "        return proxies\n",
    "\n",
    "\n",
    "    def obtain_proxy_list_from_tt_proxy(self,start=0,end=-1):\n",
    "        proxies_list = self.get_proxy_list_from_tt_proxy()\n",
    "        final_proxies = []\n",
    "        for PROXY in proxies_list:\n",
    "            if self.flag:\n",
    "                pr = {\"http\"  : \"http://\" + PROXY}\n",
    "            else:\n",
    "                pr = {\"http\"  : \"http://\" + PROXY,\"https\" : \"http://\" + PROXY}\n",
    "            final_proxies.append(pr)\n",
    "        self.proxies_list = final_proxies\n",
    "\n",
    "\n",
    "    def add_ip_to_whitelist(self,ip=''):\n",
    "        secret = '50piVRZ9ZedEy2EfqXX9u3'\n",
    "        params = {\n",
    "        \"license\": \"PBA5EE66C8FE5DE84\",\n",
    "        \"time\": int(time.time()),\n",
    "        \"cnt\": 1000,\n",
    "        }\n",
    "        params[\"sign\"] = hashlib.md5((params[\"license\"] + str(params[\"time\"]) + secret).encode('utf-8')).hexdigest()\n",
    "        params['ip']=ip\n",
    "        response = requests.get(\n",
    "                url='https://api.ttproxy.com/v1/whitelist/add',\n",
    "                params=params,\n",
    "                headers={\n",
    "                    \"Content-Type\": \"text/plain; charset=utf-8\",\n",
    "                },\n",
    "                data=\"1\"\n",
    "            ).json()\n",
    "        return response\n",
    "\n",
    "    def find_my_ip_address(self):\n",
    "        ## importing socket module\n",
    "        import socket\n",
    "        ## getting the hostname by socket.gethostname() method\n",
    "        hostname = socket.gethostname()\n",
    "        ## getting the IP address using socket.gethostbyname() method\n",
    "        ip_address = socket.gethostbyname(hostname)\n",
    "        ## printing the hostname and ip_address\n",
    "        # print(f\"Hostname: {hostname}\")\n",
    "        # print(f\"IP Address: {ip_address}\")\n",
    "        return ip_address\n",
    "\n",
    "    def get_ip(self):\n",
    "        import socket\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        try:\n",
    "            # doesn't even have to be reachable\n",
    "            s.connect(('10.255.255.255', 1))\n",
    "            IP = s.getsockname()[0]\n",
    "        except Exception:\n",
    "            IP = '127.0.0.1'\n",
    "        finally:\n",
    "            s.close()\n",
    "        return IP\n",
    "\n",
    "    def get_ip_API(self):\n",
    "        api_url = 'https://api.ipify.org?format=json'\n",
    "        return requests.get(api_url).json()['ip']\n",
    "\n",
    "    def get_ip_address_by_google(self):\n",
    "        from requests_html import HTMLSession\n",
    "        from bs4 import BeautifulSoup\n",
    "        sess = HTMLSession()\n",
    "        rrr = \"https://www.google.com/search?ei=uihSYKirGpPB3LUP_Li3gAk&q=what+is+my+ip\"\n",
    "        res = sess.get(rrr).content\n",
    "        res = BeautifulSoup(res,\"lxml\")\n",
    "        res = res.find(\"span\",style=\"font-size:20px\").text\n",
    "        return res\n",
    "\n",
    "\n",
    "def find(key, dictionary):\n",
    "    if isinstance(dictionary, dict):\n",
    "        for k, v in dictionary.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            elif isinstance(v, dict):\n",
    "                for result in find(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, list):\n",
    "                for d in v:\n",
    "                    for result in find(key, d):\n",
    "                        yield result\n",
    "    else:\n",
    "        if isinstance(dictionary, list):\n",
    "            for d in dictionary:\n",
    "                for result in find(key, d):\n",
    "                    yield result\n",
    "\n",
    "\n",
    "def get_images_from_hashtag_page(hashtag=\"nightsky\", scroll_page=3):\n",
    "    hashtag_url = f\"https://www.instagram.com/explore/tags/{hashtag}/?__a=1\"\n",
    "    next_page_url = \"\"\n",
    "    complete_url = hashtag_url+next_page_url\n",
    "    goto_next_page = True\n",
    "    image_urls_list = []\n",
    "    while goto_next_page and scroll_page > 0:\n",
    "        complete_url = hashtag_url+next_page_url\n",
    "        #response = requests.get(complete_url,headers=h).json()\n",
    "        response = cr.get(complete_url, headers=h, parser='json')\n",
    "#         response = requests.get(complete_url).json()\n",
    "        page_info = response['graphql']['hashtag'][\"edge_hashtag_to_media\"][\"page_info\"]\n",
    "        if page_info[\"has_next_page\"]:\n",
    "            end_cursor = page_info[\"end_cursor\"]\n",
    "            complete_url = \"&max_id={}\".format(end_cursor)\n",
    "        else:\n",
    "            goto_next_page = False\n",
    "        edges = response['graphql']['hashtag'][\"edge_hashtag_to_media\"][\"edges\"]\n",
    "        image_urls = [a[\"node\"][\"display_url\"] for a in edges]\n",
    "        image_urls_list = image_urls_list+image_urls\n",
    "        scroll_page = scroll_page - 1\n",
    "        time.sleep(random.randint(10,20))\n",
    "    return list(set(image_urls_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = Custom_Requests(use_proxy_param=True, use_session=True)\n",
    "cr.proxy_handler.IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igs = get_images_from_hashtag_page(\"toonapp\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today date is:  2022-03-22\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "print(\"Today date is: \", today)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/sheets/data-{today}.txt\", 'w') as f:\n",
    "    for s in igs:\n",
    "        f.write(str(s) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = '2022-03-22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/sheets/data-{today}.txt\", 'r') as f:\n",
    "    data = [line.rstrip('\\n') for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for img in data:       \n",
    "    urllib.request.urlretrieve(\n",
    "        img, \"00000001.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
